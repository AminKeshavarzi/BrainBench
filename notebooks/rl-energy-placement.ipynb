{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs to Design a Reinforcement Learning Algorithm for Energy-Efficient Workload Placement in Data Centers\n",
    "\n",
    "In this notebook, we explore how Large Language Models (LLMs) can assist in designing reinforcement learning (RL) algorithms for a practical and impactful use case: **energy-efficient workload placement in data centers**.\n",
    "\n",
    "Our approach involves prompting multiple state-of-the-art LLMs to act as AI researchers and collaboratively propose RL-based solutions to this problem. The models queried include:\n",
    "\n",
    "- **GPT-4o-mini**\n",
    "- **Gemini 2.0 Flash**\n",
    "- **DeepSeek Chat**\n",
    "- **DeepSeek Reasoner**\n",
    "- **LLaMA 3.3 70B Versatile**\n",
    "- **LLaMA 3.2**\n",
    "\n",
    "Each of these models was tasked with generating an RL algorithm tailored to the energy-aware scheduling of workloads within a data center environment.\n",
    "\n",
    "To evaluate the quality and viability of the responses, we used **o3-mini**, a compact model optimized for evaluation tasks, to critically assess the outputs across key dimensions.\n",
    "\n",
    "At the end of this notebook, I will present a concise summary of the findings, highlighting how each model contributed to the overall solution space and reflecting on the effectiveness of LLMs in automating or accelerating the design of RL algorithms for real-world infrastructure optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. Use LLMs like GPT-4o-mini, Gemini, DeepSeek, and LLaMA to design RL solutions.\n",
    "2. Provide each LLM the same prompt and collect their responses.\n",
    "3. Evaluate them using a separate model (O3-mini) based on quality and reasoning.\n",
    "4. Compare their runtime and quality of output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Packages\n",
    "\n",
    "The following cell imports all necessary libraries and modules used throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key Management\n",
    "\n",
    "To access different LLMs, you need API keys for each provider. Some APIs are free to use, while others require a paid subscription.  \n",
    "For security and convenience, all API keys are stored in a `.env` file.\n",
    "\n",
    "The following cell loads the keys from the `.env` file so they can be used throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the OpenAI  key:sk-p\n",
      "This is the Gimini key:AIza\n",
      "This is the GROQ key:gsk_\n",
      "This is the DeepSeek key:sk-e\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"This is the OpenAI  key:{openai_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"No API key found for OpenAI\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"This is the Gimini key:{google_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"No API key found for Google\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"This is the GROQ key:{groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"No API key found for Groq\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"This is the DeepSeek key:{deepseek_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"No API key found for DeepSeek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting the LLMs\n",
    "\n",
    "We craft a detailed prompt instructing the LLMs to act as researchers and design an RL-based solution. The LLMs are expected to define:\n",
    "- The environment (observation/action space)\n",
    "- The agent\n",
    "- Suitable algorithms\n",
    "- And constraints like node capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"\"\"\n",
    "You are a computer science researcher. Your task is to design an RL system to optimize energy in a datacenter. To complete the task, you need to follow these steps:\n",
    "1. Design the cluster environment: Workloads arrive one by one and send their requests, including the demanded amounts of CPU, Memory, and Storage. These workloads are placed on a node and, after some time, leave the system. Therefore, you should define the observation space, action space, and the reset and step functions.\n",
    "2. Design the reward function\n",
    "3. Design the agent\n",
    "4. Select the best algorithms for this problem\n",
    "The amount of demanded resources for all workloads running on a node should not exceed the capacity of that node.\n",
    "give me the answer in scientific tone and in markdown format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Variables and Setting Up the LLM Request\n",
    "\n",
    "- `competitors`, `answers`, and `times` are lists to store different LLMsâ€™ names, their generated responses, and the running time.  \n",
    "- `openai = OpenAI()` initializes the OpenAI client to interact with the GPT models.  \n",
    "- `messages` defines the input prompt to be sent to the LLM, specifying the user role and task description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "times = []\n",
    "openai = OpenAI()\n",
    "messages = [{\"role\": \"user\", \"content\": task_description}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the GPT-4o and Storing Its Response\n",
    "\n",
    "- Set the target model with `model_name`.  \n",
    "- Send the prompt (`messages`) to the model using the OpenAI client.  \n",
    "- *(The following steps repeat the same for a different model.)*\n",
    "- Extract the generated answer from the response.  \n",
    "- Display the answer in Markdown format for readability.  \n",
    "- Append the model name and its answer to the respective lists for later comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 10.346619367599487\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Designing a Reinforcement Learning System for Energy Optimization in a Data Center\n",
       "\n",
       "## 1. Cluster Environment Design\n",
       "\n",
       "### 1.1 Observation Space\n",
       "The observation space should capture the current state of the datacenter. It could include the following components:\n",
       "- **Node States**: \n",
       "  - CPU utilization (as a percentage)\n",
       "  - Memory utilization (as a percentage)\n",
       "  - Storage utilization (as a percentage)\n",
       "  - Current temperature of the node\n",
       "  - Power consumption of the node in Watts\n",
       "  \n",
       "- **Workload Queue**:\n",
       "  - Number of pending workloads\n",
       "  - Attributes of the workloads (e.g., required CPU, Memory, Storage for each workload)\n",
       "\n",
       "Mathematically, the observation space can be represented as:\n",
       "\\[ \\text{Observation} = \\{ \\text{Node States}, \\text{Workload Queue} \\} \\]\n",
       "\n",
       "### 1.2 Action Space\n",
       "The action space will dictate how workloads are assigned to nodes. Actions could consist of:\n",
       "- Assigning a particular workload to a specific node\n",
       "- Moving a workload from one node to another (migration)\n",
       "- Releasing resources when a workload completes\n",
       "- Adjusting the power state of nodes (e.g., turn on/off)\n",
       "\n",
       "The action space can be defined formally as:\n",
       "\\[ \\text{Action} = \\{ \\text{Assign (workload, node)}, \\text{Migrate (workload, from node to node)}, \\text{Release (workload)}, \\text{Power State Change (node)} \\} \\]\n",
       "\n",
       "### 1.3 Reset Function\n",
       "The reset function initializes or reinitializes the state of the environment. It typically involves:\n",
       "- Setting all nodes to an idle state (0% CPU, Memory, Storage)\n",
       "- Clearing the workload queue\n",
       "- Resetting power states and thermal states\n",
       "\n",
       "### 1.4 Step Function\n",
       "The step function executes the chosen action, updates the state of the environment, and computes the reward. It operates as follows:\n",
       "1. Update the workload queue based on incoming/outgoing workloads.\n",
       "2. Allocate resources based on the selected action.\n",
       "3. Update node metrics such as CPU, Memory, Storage utilization, temperature, and power consumption.\n",
       "4. Return the new observation, reward, and a flag indicating whether the episode has ended.\n",
       "\n",
       "## 2. Reward Function\n",
       "The reward function is critical for reinforcing desirable behaviors in the RL agent. A suitable reward structure may include:\n",
       "- **Negative reward for energy consumption**: Proportional to the power consumed by all nodes during a time step.\n",
       "- **Positive reward for successful workload handling**: Allocating nodes without exceeding capacity or leading to overheating.\n",
       "- **Extra penalty for idle nodes**: To encourage resource utilization rather than idleness.\n",
       "- **Penalty for migration**: To discourage frequent workload migrations unless necessary.\n",
       "\n",
       "Thus, the reward function \\( R \\) can be expressed as:\n",
       "\\[ R = - (\\text{Total Power Consumption}) + k \\times (\\text{Successful Workloads}) - m \\times (\\text{Idle Nodes}) - n \\times (\\text{Migrations}) \\]\n",
       "Where \\( k, m, n \\) are constants that balance the importance of different factors.\n",
       "\n",
       "## 3. Agent Design\n",
       "The agent must be designed to operate effectively within the specified environment. Key components include:\n",
       "- **Policy Network**: A neural network that takes observations as inputs and outputs action distributions. This could be implemented through Deep Q-Networks (DQN) or policy gradients.\n",
       "- **Experience Replay**: To enhance learning stability, maintain an experience replay buffer for off-policy learning.\n",
       "- **Exploration Strategy**: Incorporate strategies such as Îµ-greedy to balance exploration of new actions with exploitation of known rewarding actions.\n",
       "\n",
       "## 4. Algorithm Selection\n",
       "Based on the characteristics of the problem, the following algorithms could be well-suited:\n",
       "- **Deep Q-Network (DQN)**: For discrete action spaces; leverages deep learning to approximate Q-values.\n",
       "- **Proximal Policy Optimization (PPO)**: For continuous action spaces; known for its stability and adaptability in training.\n",
       "- **Trust Region Policy Optimization (TRPO)**: Offers a more formal constraint for policy updates, ensuring that the agent does not stray excessively from prior policies.\n",
       "- **Deep Deterministic Policy Gradient (DDPG)**: Suitable for environments with continuous action spaces.\n",
       "\n",
       "Each algorithm will require careful tuning of hyperparameters and may benefit from domain-specific modifications.\n",
       "\n",
       "---\n",
       "\n",
       "This structured approach provides a systematic framework to develop an RL-based system for optimizing energy use within a datacenter, leveraging the interaction between workloads and resource management effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "start = time.time()\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "end = time.time()\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "elapsed = end - start\n",
    "print(f\"running time: {elapsed}\")\n",
    "times.append(elapsed)\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Gemini API to Get a Model Response\n",
    "\n",
    "- Initialize the Gemini client with the Google API key and the appropriate base URL.  \n",
    "- Specify the Gemini model name (`gemini-2.0-flash`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 14.083750247955322\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Reinforcement Learning System for Datacenter Energy Optimization\n",
       "\n",
       "This document outlines the design of a Reinforcement Learning (RL) system for optimizing energy consumption in a datacenter environment. The core objective is to dynamically manage workload placement to minimize energy expenditure while ensuring Quality of Service (QoS) by adhering to resource constraints.\n",
       "\n",
       "### 1. Datacenter Environment Design\n",
       "\n",
       "The datacenter environment is modeled as a discrete-time simulation. It comprises a cluster of `N` nodes, each with a defined capacity of CPU, Memory, and Storage.  Workloads arrive dynamically, requesting specific amounts of these resources.  These requests are served by placing the workload on a suitable node. Workloads remain active for a variable duration and then depart.\n",
       "\n",
       "**1.1 Observation Space:**\n",
       "\n",
       "The observation space, `S`, needs to represent the current state of the datacenter.  It includes node utilization and workload characteristics. A potential design is:\n",
       "\n",
       "`S = [s_1, s_2, ..., s_N, w]`\n",
       "\n",
       "where:\n",
       "\n",
       "*   `s_i` represents the state of node *i*, containing:\n",
       "    *   `cpu_util_i`: CPU utilization of node *i* (percentage).\n",
       "    *   `mem_util_i`: Memory utilization of node *i* (percentage).\n",
       "    *   `storage_util_i`: Storage utilization of node *i* (percentage).\n",
       "    *   `node_temp_i`: Temperature of node *i* (Celsius). (Optional, but relevant for energy modeling).\n",
       "*   `w` represents the current workload's resource demands:\n",
       "    *   `cpu_req`: CPU requested by the current workload.\n",
       "    *   `mem_req`: Memory requested by the current workload.\n",
       "    *   `storage_req`: Storage requested by the current workload.\n",
       "    *   `duration_req`: Duration of the workload (discrete time steps). (Optional, for workload scheduling).\n",
       "\n",
       "The observation space is continuous (for utilization percentages and resource demands) and potentially high-dimensional, depending on the number of nodes.  Normalization techniques (e.g., min-max scaling or standardization) are crucial to improve learning stability.\n",
       "\n",
       "**1.2 Action Space:**\n",
       "\n",
       "The action space, `A`, represents the possible actions the agent can take, which is the node where to place the incoming workload.\n",
       "\n",
       "`A = {0, 1, 2, ..., N-1}`\n",
       "\n",
       "where each integer represents the index of a node in the cluster. Action `i` corresponds to placing the workload on node *i*. If no feasible node is available (i.e., placing the workload on any node would exceed its capacity), a designated \"reject\" action `N` could be included.\n",
       "\n",
       "**1.3 Reset Function:**\n",
       "\n",
       "The `reset()` function initializes the datacenter environment to a clean state. This includes:\n",
       "\n",
       "*   Setting all node utilizations to zero.\n",
       "*   Generating a new, random workload according to a predefined workload arrival distribution (e.g., Poisson process).\n",
       "*   Setting the simulation time to zero.\n",
       "\n",
       "**1.4 Step Function:**\n",
       "\n",
       "The `step(action)` function executes an action and updates the environment. It comprises the following steps:\n",
       "\n",
       "1.  **Action Execution:**\n",
       "    *   If `action` is `0` to `N-1`: Attempt to place the current workload on node `action`.  Check if sufficient resources are available (CPU, Memory, Storage) on the target node.\n",
       "    *   If `action` is `N` (the \"reject\" action): Reject the current workload.\n",
       "\n",
       "2.  **Resource Allocation (if action successful):**\n",
       "    *   Update node utilizations based on the workload's resource demands.\n",
       "\n",
       "3.  **Reward Calculation (described in Section 2).**\n",
       "\n",
       "4.  **Environment Update:**\n",
       "    *   Advance the simulation time by one time step.\n",
       "    *   Decrement the remaining duration of currently running workloads.\n",
       "    *   Remove workloads that have completed their execution (duration reaches zero).\n",
       "    *   Generate a new workload according to the workload arrival distribution.\n",
       "\n",
       "5.  **Observation Update:**\n",
       "    *   Update the observation space `S` based on the new node utilizations and the new workload.\n",
       "\n",
       "6.  **Termination Condition:** The simulation episode terminates after a fixed number of time steps or when a critical failure occurs (e.g., a node overheats).\n",
       "\n",
       "7.  **Return:** The function returns `(next_state, reward, done, info)`, where:\n",
       "    *   `next_state`: The updated observation space `S`.\n",
       "    *   `reward`: The reward received for taking the action.\n",
       "    *   `done`: A boolean indicating whether the episode has terminated.\n",
       "    *   `info`:  A dictionary containing debugging information (e.g., resource allocation details, rejected workload count).\n",
       "\n",
       "### 2. Reward Function Design\n",
       "\n",
       "The reward function, `R`, is critical for guiding the RL agent towards energy-efficient workload placement.  It should incentivize low energy consumption while penalizing resource violations and workload rejections.  A potential design is:\n",
       "\n",
       "`R = w_e * R_energy + w_r * R_reject + w_v * R_violation`\n",
       "\n",
       "where:\n",
       "\n",
       "*   `R_energy`: Reward related to energy consumption.  This should be *negative* to penalize high energy usage. A suitable formulation is:\n",
       "\n",
       "    `R_energy = -Energy_consumption`\n",
       "\n",
       "    `Energy_consumption` can be estimated based on node utilization and temperature. A simplified model could be:\n",
       "\n",
       "    `Energy_consumption = sum(P_idle_i + (P_max_i - P_idle_i) * cpu_util_i / 100)` summed over all nodes `i`, where `P_idle_i` and `P_max_i` are the idle and maximum power consumption of node *i*, respectively.\n",
       "*   `R_reject`: Reward for rejecting a workload. This should also be *negative* to penalize workload rejections.\n",
       "\n",
       "    `R_reject = -Penalty_reject  if action == N else 0`\n",
       "\n",
       "    `Penalty_reject` is a predefined penalty value.  A higher penalty encourages the agent to accept workloads whenever possible.\n",
       "\n",
       "*   `R_violation`: Reward for resource violations (exceeding node capacity).  This should be a large *negative* value to strongly discourage violations.\n",
       "\n",
       "    `R_violation = -Penalty_violation if any_node_violated else 0`\n",
       "\n",
       "    `Penalty_violation` is a significantly larger penalty than `Penalty_reject`.  `any_node_violated` is a boolean indicating whether any node's resource capacity (CPU, Memory, Storage) was exceeded after placing the workload.\n",
       "\n",
       "*   `w_e`, `w_r`, and `w_v` are weights that balance the relative importance of each reward component.  These weights require careful tuning to achieve the desired performance.  A good starting point might be `w_e = 1`, `w_r = 10`, and `w_v = 100`.\n",
       "\n",
       "### 3. Agent Design\n",
       "\n",
       "The agent is a neural network that maps the observed state `S` to an action `A`.  The architecture of the neural network depends on the chosen RL algorithm (see Section 4).  Common choices include:\n",
       "\n",
       "*   **Deep Q-Network (DQN):**  The network estimates the Q-values (action-value function) for each possible action given the current state. The agent selects the action with the highest Q-value (exploitation) or a random action (exploration) according to an epsilon-greedy policy.  Requires a separate network for Q-value approximation.\n",
       "*   **Actor-Critic Methods (e.g., A2C, PPO, DDPG):** These methods employ two networks: an actor network that determines the policy (probability distribution over actions) and a critic network that estimates the value function (expected cumulative reward). They are suitable for continuous action spaces (though discretizing the action space is also possible). The actor uses the critic's feedback to improve its policy.\n",
       "\n",
       "**Agent Components:**\n",
       "\n",
       "*   **Neural Network:**  A deep neural network (DNN) with multiple layers, parameterized by weights `Î¸`. The input is the state `S`, and the output depends on the chosen RL algorithm.  For DQN, the output is a Q-value for each action. For actor-critic methods, the output includes parameters for the policy (e.g., mean and standard deviation of a Gaussian distribution for continuous actions).\n",
       "*   **Experience Replay Buffer (DQN):** A memory buffer that stores past experiences `(s, a, r, s', done)`.  The agent samples mini-batches from the buffer to train the neural network, breaking temporal correlations and improving learning stability.\n",
       "*   **Optimizer:** An optimization algorithm (e.g., Adam, RMSprop) used to update the neural network weights `Î¸` based on the gradients calculated from the reward signal.\n",
       "*   **Exploration Strategy:**  A strategy for balancing exploration (trying new actions) and exploitation (choosing the best-known action). Common strategies include epsilon-greedy (DQN) and adding noise to the action selection (DDPG).\n",
       "\n",
       "### 4. Algorithm Selection\n",
       "\n",
       "Given the nature of the problem, the following RL algorithms are considered suitable:\n",
       "\n",
       "*   **Deep Q-Network (DQN):** A classic value-based algorithm suitable for discrete action spaces. DQN is relatively straightforward to implement but can suffer from instability, especially with high-dimensional state spaces. Techniques like Double DQN, Dueling DQN, and prioritized experience replay can improve its performance.  *Justification:* Simple to implement and understand, good baseline to compare to more complex algorithms.\n",
       "\n",
       "*   **Proximal Policy Optimization (PPO):** A state-of-the-art policy gradient algorithm that balances exploration and exploitation effectively. PPO is more stable than other policy gradient methods due to its clipping mechanism, which prevents overly large policy updates. It is suitable for both discrete and continuous action spaces. *Justification:* Stable, well-performing policy gradient method suitable for complex environments.\n",
       "\n",
       "*   **Soft Actor-Critic (SAC):**  An off-policy actor-critic algorithm that incorporates entropy regularization, encouraging exploration and preventing premature convergence to suboptimal policies. SAC is known for its sample efficiency and robustness. *Justification:* Efficient and robust exploration, especially suited for complex, high-dimensional environments.\n",
       "\n",
       "**Justification for Algorithm Selection:**\n",
       "\n",
       "*   The datacenter environment is complex, with a continuous state space and a discrete action space.\n",
       "*   The reward function is sparse and delayed, making it challenging for the agent to learn.\n",
       "*   The high dimensionality of the state space requires powerful function approximation techniques, such as deep neural networks.\n",
       "\n",
       "**Training Procedure:**\n",
       "\n",
       "1.  **Initialization:** Initialize the environment, the agent (neural network and optimizer), and the experience replay buffer (if applicable).\n",
       "2.  **Episode Loop:** Repeat until the maximum number of episodes is reached:\n",
       "    *   Reset the environment to a new initial state.\n",
       "    *   **Time Step Loop:** Repeat until the episode terminates:\n",
       "        *   Observe the current state `s`.\n",
       "        *   Select an action `a` based on the agent's policy (e.g., epsilon-greedy for DQN, policy network for actor-critic methods).\n",
       "        *   Execute the action in the environment and observe the next state `s'`, the reward `r`, and the done flag.\n",
       "        *   Store the experience `(s, a, r, s', done)` in the experience replay buffer (if applicable).\n",
       "        *   Update the agent's neural network weights by sampling a mini-batch from the experience replay buffer (if applicable) and applying the chosen RL algorithm's update rule (e.g., Q-learning update for DQN, policy and value function updates for actor-critic methods).\n",
       "        *   Update the current state `s = s'`.\n",
       "3.  **Evaluation:** After training, evaluate the agent's performance on a separate test set to assess its generalization ability.\n",
       "\n",
       "**Hyperparameter Tuning:**\n",
       "\n",
       "The performance of the RL system is highly sensitive to hyperparameters, such as learning rate, discount factor, exploration rate, and neural network architecture.  Hyperparameter tuning is crucial for achieving optimal performance. Techniques like grid search, random search, or Bayesian optimization can be used to find the best hyperparameter values.\n",
       "\n",
       "This document provides a comprehensive framework for designing an RL system for datacenter energy optimization. The specific implementation details will depend on the available resources and the desired level of performance. The design choices presented here provide a solid foundation for further research and development.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "start = time.time()\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "end = time.time()\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "elapsed = end - start\n",
    "print(f\"running time: {elapsed}\")\n",
    "times.append(elapsed)\n",
    "\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying DeepSeekâ€™s Chat Model\n",
    "\n",
    "- Initialize the DeepSeek client using the corresponding API key and base URL.  \n",
    "- Set the model name to `\"deepseek-chat\"` and `\"deepseek-reasoner\"`.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 61.84269118309021\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Reinforcement Learning System for Datacenter Energy Optimization\n",
       "\n",
       "## 1. Cluster Environment Design\n",
       "\n",
       "### Observation Space\n",
       "The observation space consists of:\n",
       "- **Node states**: For each node (n âˆˆ N), we track:\n",
       "  - Available CPU (percentage)\n",
       "  - Available memory (percentage)\n",
       "  - Available storage (percentage)\n",
       "  - Current power consumption (normalized)\n",
       "  - Temperature (normalized)\n",
       "- **Workload characteristics**: For incoming workload (w):\n",
       "  - Requested CPU cores (normalized)\n",
       "  - Requested memory (GB, normalized)\n",
       "  - Requested storage (GB, normalized)\n",
       "  - Expected duration (normalized)\n",
       "- **System metrics**:\n",
       "  - Current PUE (Power Usage Effectiveness)\n",
       "  - Time of day (for workload pattern awareness)\n",
       "\n",
       "Total observation dimension: `5|N| + 4 + 2`\n",
       "\n",
       "### Action Space\n",
       "The action space is discrete with `|N| + 1` possible actions:\n",
       "- Place workload on node 1\n",
       "- Place workload on node 2\n",
       "- ...\n",
       "- Place workload on node |N|\n",
       "- Reject workload (when no suitable node available)\n",
       "\n",
       "### Environment Dynamics\n",
       "\n",
       "#### Reset Function\n",
       "```python\n",
       "def reset():\n",
       "    # Initialize all nodes to empty state\n",
       "    for node in nodes:\n",
       "        node.available_cpu = node.total_cpu\n",
       "        node.available_mem = node.total_mem\n",
       "        node.available_storage = node.total_storage\n",
       "        node.power = idle_power\n",
       "        node.temperature = ambient_temp\n",
       "    \n",
       "    # Clear workload queue\n",
       "    workload_queue.clear()\n",
       "    \n",
       "    # Reset metrics\n",
       "    metrics = {\n",
       "        'total_energy': 0,\n",
       "        'rejected_workloads': 0,\n",
       "        'pue': 1.0\n",
       "    }\n",
       "    \n",
       "    # Generate initial observation\n",
       "    return get_observation()\n",
       "```\n",
       "\n",
       "#### Step Function\n",
       "```python\n",
       "def step(action):\n",
       "    # Get current workload\n",
       "    workload = workload_queue.pop(0)\n",
       "    \n",
       "    if action < len(nodes):  # Placement action\n",
       "        node = nodes[action]\n",
       "        \n",
       "        # Check resource constraints\n",
       "        if (node.available_cpu >= workload.cpu and\n",
       "            node.available_mem >= workload.mem and\n",
       "            node.available_storage >= workload.storage):\n",
       "            \n",
       "            # Update node resources\n",
       "            node.available_cpu -= workload.cpu\n",
       "            node.available_mem -= workload.mem\n",
       "            node.available_storage -= workload.storage\n",
       "            \n",
       "            # Calculate power increase\n",
       "            power_delta = calculate_power_increase(node, workload)\n",
       "            node.power += power_delta\n",
       "            \n",
       "            # Schedule workload completion\n",
       "            schedule_completion(node, workload)\n",
       "            \n",
       "            reward = calculate_reward(node, workload, 'placed')\n",
       "        else:\n",
       "            reward = calculate_reward(None, workload, 'rejected')\n",
       "    else:  # Reject action\n",
       "        reward = calculate_reward(None, workload, 'rejected')\n",
       "    \n",
       "    # Get next workload\n",
       "    workload_queue.append(generate_workload())\n",
       "    \n",
       "    # Update system metrics\n",
       "    update_metrics()\n",
       "    \n",
       "    # Check termination\n",
       "    done = check_termination()\n",
       "    \n",
       "    return get_observation(), reward, done, get_metrics()\n",
       "```\n",
       "\n",
       "## 2. Reward Function Design\n",
       "\n",
       "The reward function balances multiple objectives:\n",
       "\n",
       "```python\n",
       "def calculate_reward(node, workload, action_type):\n",
       "    if action_type == 'rejected':\n",
       "        return -Î±  # Penalty for rejection\n",
       "    \n",
       "    # Energy efficiency components\n",
       "    power_reward = -Î² * (node.power / node.max_power)\n",
       "    pue_reward = -Î³ * (current_pue - ideal_pue)\n",
       "    \n",
       "    # Resource utilization components\n",
       "    util_reward = Î´ * (\n",
       "        (1 - node.available_cpu/node.total_cpu) +\n",
       "        (1 - node.available_mem/node.total_mem) +\n",
       "        (1 - node.available_storage/node.total_storage)\n",
       "    ) / 3\n",
       "    \n",
       "    # Temperature penalty\n",
       "    temp_penalty = -Îµ * max(0, (node.temperature - threshold_temp)/threshold_temp)\n",
       "    \n",
       "    # Workload affinity bonus (if applicable)\n",
       "    affinity_bonus = Î· * check_affinity(node, workload)\n",
       "    \n",
       "    total_reward = (\n",
       "        power_reward + \n",
       "        pue_reward + \n",
       "        util_reward + \n",
       "        temp_penalty + \n",
       "        affinity_bonus\n",
       "    )\n",
       "    \n",
       "    return total_reward\n",
       "```\n",
       "\n",
       "Where Î±, Î², Î³, Î´, Îµ, Î· are tunable hyperparameters that control the trade-off between:\n",
       "- Workload acceptance rate\n",
       "- Energy efficiency\n",
       "- Resource utilization\n",
       "- Thermal management\n",
       "- Workload affinity considerations\n",
       "\n",
       "## 3. Agent Design\n",
       "\n",
       "We propose a hybrid agent architecture:\n",
       "\n",
       "### Feature Extraction Module\n",
       "- Multi-layer perceptron for processing node states\n",
       "- Embedding layer for workload characteristics\n",
       "- Temporal attention mechanism for time-dependent patterns\n",
       "\n",
       "### Policy Network\n",
       "- Dueling DQN architecture for value estimation\n",
       "- Separate streams for state value and action advantages\n",
       "- Ïµ-greedy exploration with adaptive decay\n",
       "\n",
       "### Experience Replay\n",
       "- Prioritized experience replay buffer\n",
       "- Importance sampling for bias correction\n",
       "- Hindsight experience replay for rare events\n",
       "\n",
       "### Auxiliary Tasks\n",
       "- Joint learning of:\n",
       "  - Node power consumption prediction\n",
       "  - Workload duration prediction\n",
       "  - Temperature forecasting\n",
       "\n",
       "## 4. Algorithm Selection\n",
       "\n",
       "### Baseline Algorithms\n",
       "1. **Deep Q-Network (DQN)**\n",
       "   - Suitable for discrete action space\n",
       "   - Stable learning with experience replay\n",
       "   - Extensible with double Q-learning and dueling architecture\n",
       "\n",
       "2. **Proximal Policy Optimization (PPO)**\n",
       "   - Policy gradient method with clipped updates\n",
       "   - Better for handling continuous aspects of the problem\n",
       "   - More sample-efficient than vanilla policy gradients\n",
       "\n",
       "### Advanced Options\n",
       "1. **Multi-Agent RL (MADRLN)**\n",
       "   - Treat each node as an independent agent\n",
       "   - Centralized training with decentralized execution\n",
       "   - Uses attention mechanisms for coordination\n",
       "\n",
       "2. **Hierarchical RL**\n",
       "   - High-level controller for workload distribution\n",
       "   - Low-level controllers for individual node optimization\n",
       "   - Temporal abstraction improves scalability\n",
       "\n",
       "3. **Model-Based RL**\n",
       "   - Learn transition dynamics of the datacenter\n",
       "   - Combine with model predictive control\n",
       "   - Particularly effective for thermal management\n",
       "\n",
       "### Recommended Approach\n",
       "A **Rainbow DQN** variant combining:\n",
       "- Double Q-learning\n",
       "- Prioritized experience replay\n",
       "- Dueling networks\n",
       "- Multi-step learning\n",
       "- Distributional RL\n",
       "- Noisy nets for exploration\n",
       "\n",
       "This provides state-of-the-art performance while maintaining relative implementation simplicity compared to more complex actor-critic methods.\n",
       "\n",
       "## Implementation Considerations\n",
       "\n",
       "1. **Simulation Fidelity**:\n",
       "   - Calibrate power models using real datacenter measurements\n",
       "   - Validate workload patterns against production traces\n",
       "\n",
       "2. **Training Protocol**:\n",
       "   - Curriculum learning starting with simple scenarios\n",
       "   - Progressive neural networks for transfer learning across configurations\n",
       "\n",
       "3. **Safety Mechanisms**:\n",
       "   - Action masking to prevent invalid placements\n",
       "   - Runtime constraints on decision latency\n",
       "   - Fallback to heuristic methods during exploration\n",
       "\n",
       "4. **Evaluation Metrics**:\n",
       "   - Energy efficiency (PUE improvement)\n",
       "   - Workload completion rate\n",
       "   - Resource utilization variance\n",
       "   - Thermal violation frequency\n",
       "   - Decision latency percentiles"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "start = time.time()\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "end = time.time()\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "elapsed = end - start\n",
    "print(f\"running time: {elapsed}\")\n",
    "times.append(elapsed)\n",
    "\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 214.10724592208862\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Reinforcement Learning System for Datacenter Energy Optimization\n",
       "\n",
       "#### 1. **Cluster Environment Design**\n",
       "The environment simulates a datacenter cluster with `N` heterogeneous nodes. Workloads arrive sequentially, each characterized by a resource demand vector and duration. Nodes have finite capacity for CPU, Memory, and Storage.\n",
       "\n",
       "##### **State Space (`observation_space`)**  \n",
       "- **Node States**:  \n",
       "  For each node `i` (1 â‰¤ `i` â‰¤ `N`):  \n",
       "  - Normalized remaining CPU: `[0, 1]`  \n",
       "  - Normalized remaining Memory: `[0, 1]`  \n",
       "  - Normalized remaining Storage: `[0, 1]`  \n",
       "- **Workload State**:  \n",
       "  - Normalized demanded CPU: `[0, 1]`  \n",
       "  - Normalized demanded Memory: `[0, 1]`  \n",
       "  - Normalized demanded Storage: `[0, 1]`  \n",
       "- **Time State**:  \n",
       "  - Normalized time since workload arrival: `[0, 1]`  \n",
       "- **Dimensionality**:  \n",
       "  `(N Ã— 3) + 3 + 1 = 3N + 4` continuous features.  \n",
       "\n",
       "##### **Action Space (`action_space`)**  \n",
       "- Discrete action space of size `N + 1`:  \n",
       "  - Actions `{0, 1, ..., N-1}`: Place workload on node `i`.  \n",
       "  - Action `N`: Reject workload.  \n",
       "- **Constraints**:  \n",
       "  Action `a` is invalid if `node_resources[a] < workload_demand` (unless `a = N`).\n",
       "\n",
       "##### **Reset Function**  \n",
       "```python\n",
       "def reset(self):\n",
       "    # Initialize all nodes with full resources\n",
       "    self.node_resources = np.copy(self.node_capacities)  # Shape: [N, 3]\n",
       "    # Generate first workload\n",
       "    self.workload = self._generate_workload()\n",
       "    self.arrival_time = 0.0\n",
       "    # Return initial state\n",
       "    return self._get_state()\n",
       "```\n",
       "\n",
       "##### **Step Function**  \n",
       "```python\n",
       "def step(self, action):\n",
       "    # 1. Validate action\n",
       "    if action < self.n_nodes:\n",
       "        assert np.all(self.workload.demand <= self.node_resources[action]), \"Invalid placement\"\n",
       "    \n",
       "    # 2. Update cluster state\n",
       "    if action < self.n_nodes:  # Placement\n",
       "        self.node_resources[action] -= self.workload.demand\n",
       "        self.running_workloads.append((action, self.workload))\n",
       "    # (Rejection requires no state change)\n",
       "    \n",
       "    # 3. Simulate time progression\n",
       "    self.current_time += self.timestep\n",
       "    completed = self._remove_completed_workloads()  # Free resources\n",
       "    self.workload = self._generate_workload()  # New workload\n",
       "    \n",
       "    # 4. Compute reward, next state, and done\n",
       "    reward = self._calculate_reward(action, completed)\n",
       "    next_state = self._get_state()\n",
       "    done = (self.current_time >= self.max_episode_time)\n",
       "    \n",
       "    return next_state, reward, done, {}\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "#### 2. **Reward Function**  \n",
       "The reward balances energy efficiency, workload rejection penalties, and SLA compliance:  \n",
       "\n",
       "$$r_t = -\\underbrace{\\sum_{i=1}^{N} P_i(\\mathbf{u}_i) \\cdot \\Delta t}_{\\text{Energy cost}} - \\underbrace{\\lambda_r \\cdot \\mathbb{I}_{\\text{reject}}}_{\\text{Rejection penalty}} + \\underbrace{\\lambda_s \\cdot \\mathbb{I}_{\\text{SLA}}}_{\\text{SLA bonus}}$$  \n",
       "\n",
       "- **Energy Cost**:  \n",
       "  $P_i(\\mathbf{u}_i) = P_{\\text{idle}} + (P_{\\text{max}} - P_{\\text{idle}}) \\cdot \\|\\mathbf{u}_i\\|_2$  \n",
       "  where $\\mathbf{u}_i$ is the utilization vector (CPU, Mem, Storage) of node $i$.  \n",
       "- **Rejection Penalty**:  \n",
       "  $\\lambda_r$ (e.g., 100) applied when workload is rejected.  \n",
       "- **SLA Bonus**:  \n",
       "  $\\lambda_s$ (e.g., 10) if workload completes within its deadline.  \n",
       "- **Time Step**: $\\Delta t$ (e.g., 1 second).  \n",
       "\n",
       "---\n",
       "\n",
       "#### 3. **Agent Design**  \n",
       "##### **Policy Architecture**  \n",
       "- **Input Layer**: `3N + 4` units (state features).  \n",
       "- **Hidden Layers**:  \n",
       "  - 256-unit Dense (ReLU)  \n",
       "  - 128-unit Dense (ReLU)  \n",
       "- **Output Layer**: `N + 1` units (Softmax for action probabilities).  \n",
       "\n",
       "##### **Key Components**  \n",
       "- **Experience Replay**: Stores transitions `(s, a, r, s')` to decorrelate updates.  \n",
       "- **Target Network**: Stabilizes Q-learning (if using DQN variant).  \n",
       "- **Constraints Handling**:  \n",
       "  - Mask invalid actions (e.g., nodes with insufficient resources) during training/inference.  \n",
       "\n",
       "---\n",
       "\n",
       "#### 4. **Algorithm Selection**  \n",
       "##### **Proximal Policy Optimization (PPO)**  \n",
       "- **Advantages**:  \n",
       "  - Sample-efficient and stable for high-dimensional state spaces.  \n",
       "  - Handles continuous and discrete actions via policy gradient.  \n",
       "  - Supports constraint masking natively.  \n",
       "- **Training Objective**:  \n",
       "  $$L^{\\text{CLIP}} = \\mathbb{E}_t \\left[ \\min\\left( \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}_t, \\text{clip}\\left(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t \\right) \\right]$$  \n",
       "\n",
       "##### **Alternative: Attention-based Actor-Critic**  \n",
       "- **Graph Representation**:  \n",
       "  Nodes as vertices, workloads as context.  \n",
       "- **Multi-Head Attention**:  \n",
       "  Captures node-workload affinity for placement decisions.  \n",
       "- **Advantage**:  \n",
       "  Generalizes to variable cluster sizes.  \n",
       "\n",
       "##### **Hybrid Training**  \n",
       "1. **Imitation Learning**:  \n",
       "   - Pre-train with heuristic rules (e.g., Best-Fit, First-Fit).  \n",
       "2. **RL Fine-tuning**:  \n",
       "   - PPO with KL-divergence penalty to prevent policy collapse.  \n",
       "\n",
       "---\n",
       "\n",
       "### **System Summary**  \n",
       "| **Component**       | **Design Choice**                                  |\n",
       "|----------------------|---------------------------------------------------|\n",
       "| **State Space**      | Normalized node resources + workload demand + time |\n",
       "| **Action Space**     | Discrete placement/rejection (`N+1` actions)       |\n",
       "| **Reward**           | Energy cost + rejection penalty + SLA bonus        |\n",
       "| **Agent**            | PPO with masked invalid actions                    |\n",
       "| **Optimization**     | Attention mechanisms for scalability              |\n",
       "\n",
       "**Key Innovations**:  \n",
       "- **Multi-resource utilization norm** in energy modeling.  \n",
       "- **SLA-awareness** via deadline tracking in workload lifecycle.  \n",
       "- **Action masking** ensures resource constraints are never violated.  \n",
       "\n",
       "This design optimizes energy while maintaining workload QoS, leveraging modern RL algorithms for scalable datacenter control."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"deepseek-reasoner\"\n",
    "\n",
    "start = time.time()\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "end = time.time()\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "elapsed = end - start\n",
    "print(f\"running time: {elapsed}\")\n",
    "times.append(elapsed)\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Groqâ€™s LLM Model\n",
    "\n",
    "- Initialize the Groq client using the appropriate API key and base URL.  \n",
    "- Set the model name to `\"llama-3.3-70b-versatile\"`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 5.06576681137085\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Designing a Reinforcement Learning (RL) System for Optimizing Energy in a Datacenter\n",
       "====================================================================\n",
       "\n",
       "### Step 1: Designing the Cluster Environment\n",
       "\n",
       "The cluster environment is modeled as a Markov Decision Process (MDP), which consists of the following components:\n",
       "\n",
       "* **Observation Space**: The observation space represents the current state of the datacenter. It includes:\n",
       "\t+ The current utilization of each node (CPU, Memory, Storage)\n",
       "\t+ The number of running workloads on each node\n",
       "\t+ The demanded resources (CPU, Memory, Storage) of the new arriving workload\n",
       "\t+ The available capacity of each node (CPU, Memory, Storage)\n",
       "* **Action Space**: The action space represents the possible decisions that can be made by the agent. It includes:\n",
       "\t+ Selecting a node to place the new workload\n",
       "\t+ Rejecting the new workload (e.g., due to insufficient resources)\n",
       "* **Reset Function**: The reset function is used to initialize the environment at the beginning of each episode. It includes:\n",
       "\t+ Initializing the nodes with their maximum capacity\n",
       "\t+ Setting the number of running workloads on each node to zero\n",
       "* **Step Function**: The step function is used to transition the environment from one state to another. It includes:\n",
       "\t+ Updating the utilization of each node based on the new workload placement\n",
       "\t+ Updating the number of running workloads on each node\n",
       "\t+ Checking for any node overutilization and taking corrective actions (e.g., rejecting new workloads)\n",
       "\n",
       "### Step 2: Designing the Reward Function\n",
       "\n",
       "The reward function is designed to encourage energy-efficient workload placement. It includes:\n",
       "\n",
       "* **Energy Consumption**: The energy consumption of each node is calculated based on its utilization\n",
       "* **Resource Utilization**: The utilization of each resource (CPU, Memory, Storage) is calculated for each node\n",
       "* **Overutilization Penalty**: A penalty is imposed when the demand for resources exceeds the capacity of a node\n",
       "* **Reward Calculation**: The reward is calculated as a weighted sum of the negative energy consumption, resource utilization, and overutilization penalty\n",
       "\n",
       "The reward function can be formulated as:\n",
       "\n",
       "R = - (Î± \\* Energy_Consumption + Î² \\* Resource_Utilization + Î³ \\* Overutilization_Penalty)\n",
       "\n",
       "where Î±, Î², and Î³ are weighting factors that can be adjusted to prioritize different objectives.\n",
       "\n",
       "### Step 3: Designing the Agent\n",
       "\n",
       "The agent is designed to learn an optimal policy for workload placement. It includes:\n",
       "\n",
       "* **State Representation**: The agent receives the current state of the environment as input\n",
       "* **Action Selection**: The agent selects an action based on its policy\n",
       "* **Learning**: The agent updates its policy based on the reward received from the environment\n",
       "\n",
       "### Step 4: Selecting the Best Algorithms for this Problem\n",
       "\n",
       "Several RL algorithms can be applied to this problem, including:\n",
       "\n",
       "* **Deep Q-Networks (DQN)**: A value-based algorithm that learns to estimate the expected return for each state-action pair\n",
       "* **Policy Gradient Methods (PGMs)**: A policy-based algorithm that learns to optimize the policy directly\n",
       "* **Proximal Policy Optimization (PPO)**: A model-free, on-policy algorithm that learns to optimize the policy using trust region optimization\n",
       "* **Deep Deterministic Policy Gradients (DDPG)**: A model-free, off-policy algorithm that learns to optimize the policy using actor-critic methods\n",
       "\n",
       "The choice of algorithm depends on the specific requirements of the problem, such as the size of the state and action spaces, the complexity of the reward function, and the availability of computational resources.\n",
       "\n",
       "Example Use Case\n",
       "---------------\n",
       "\n",
       "Suppose we have a datacenter with 10 nodes, each with a maximum capacity of 100 CPU, 100 Memory, and 100 Storage. We want to optimize the energy consumption of the datacenter while ensuring that the demand for resources does not exceed the capacity of each node. We can use the designed RL system to learn an optimal policy for workload placement.\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "\n",
       "# Define the observation space\n",
       "observation_space = {\n",
       "    'node_utilization': np.array([0.0, 0.0, 0.0]),  # CPU, Memory, Storage\n",
       "    'num_running_workloads': 0,\n",
       "    'new_workload_demands': np.array([0.0, 0.0, 0.0]),  # CPU, Memory, Storage\n",
       "    'available_capacity': np.array([100.0, 100.0, 100.0])  # CPU, Memory, Storage\n",
       "}\n",
       "\n",
       "# Define the action space\n",
       "action_space = {\n",
       "    'node_selection': np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),  # 10 nodes\n",
       "    'reject_workload': 0\n",
       "}\n",
       "\n",
       "# Define the reward function\n",
       "def reward_function(state, action):\n",
       "    energy_consumption = np.sum(state['node_utilization'] * state['available_capacity'])\n",
       "    resource_utilization = np.sum(state['node_utilization'])\n",
       "    overutilization_penalty = 0\n",
       "    if np.any(state['node_utilization'] > state['available_capacity']):\n",
       "        overutilization_penalty = 1\n",
       "    reward = - (0.5 * energy_consumption + 0.3 * resource_utilization + 0.2 * overutilization_penalty)\n",
       "    return reward\n",
       "\n",
       "# Define the agent\n",
       "class Agent(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(Agent, self).__init__()\n",
       "        self.fc1 = nn.Linear(10, 128)  # input layer (10) -> hidden layer (128)\n",
       "        self.fc2 = nn.Linear(128, 10)  # hidden layer (128) -> output layer (10)\n",
       "\n",
       "    def forward(self, x):\n",
       "        x = torch.relu(self.fc1(x))  # activation function for hidden layer\n",
       "        x = self.fc2(x)\n",
       "        return x\n",
       "\n",
       "# Train the agent\n",
       "agent = Agent()\n",
       "criterion = nn.MSELoss()\n",
       "optimizer = optim.Adam(agent.parameters(), lr=0.001)\n",
       "```\n",
       "\n",
       "Note that this is a simplified example and may not represent a real-world datacenter scenario. The designed RL system can be extended and modified to accommodate more complex scenarios and requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "start = time.time()\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "end = time.time()\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "elapsed = end - start\n",
    "print(f\"running time: {elapsed}\")\n",
    "times.append(elapsed)\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 43.58457088470459\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Designing an RL System for Optimizing Energy in a Datacenter\n",
       "===========================================================\n",
       "\n",
       "### Cluster Environment\n",
       "\n",
       "The cluster environment consists of multiple nodes, each with limited capacities for CPU, Memory, and Storage. Workloads arrive one by one and are placed on a node until they leave the system.\n",
       "\n",
       "#### Observation Space\n",
       "\n",
       "The observation space includes the following features:\n",
       "\n",
       "*   **Node IDs**: Unique identifier for each node in the cluster.\n",
       "*   **Available Capacity**: Remaining capacity of each resource at each node (CPU, Memory, Storage).\n",
       "*   **Workload ID**: Unique identifier for each incoming workload.\n",
       "*   **Load Levels**: Numerical representation of the load level on each node.\n",
       "\n",
       "#### Action Space\n",
       "\n",
       "The action space includes two types of actions:\n",
       "\n",
       "*   **allocateResources**: Allocate resources to a new workload, attempting not to exceed the capacity limits of the node.\n",
       "*   **assignWorkload**: Redirect an already allocated workload to another node with more available resources.\n",
       "*   **releaseResources**: Release excess resources back into the pool, without assigning them to any specific workload.\n",
       "\n",
       "#### Reset Function\n",
       "\n",
       "Upon receiving a new workload arrival (workload's timestamp), reset:\n",
       "\n",
       "*   Initialize Node availability tracking based on workload arriving.\n",
       "\n",
       "#### Step Function\n",
       "\n",
       "On each time step (`t`):\n",
       "\n",
       "1.  Check capacity constraint for allocating Resources of incoming Workload\n",
       "2.  Calculate Total Power consumed based on total load levels.\n",
       "3.  Decide based on the available capacity of nodes\n",
       "\n",
       "### Reward Function design\n",
       "\n",
       "The purpose of a reward function is to guide learning towards optimal policy.\n",
       "\n",
       "Rewards are assigned as follows:\n",
       "\n",
       "*   Positive Rewards (`Reward Positivity`) **when** resource usage increases by allocating for newly arriving workload in node and does not exceed capacity,\n",
       "*   Negative Rewards: when new load levels go beyond current allocated load from newly coming Workloads,\n",
       "\n",
       "### Agent Design\n",
       "\n",
       "A combined Q-learning, Value iteration approaches  can be used here as it will handle learning optimal load balancing policy that would result into low energy costs.\n",
       "\n",
       "### Algorithm selection:\n",
       "\n",
       "Given the specifics of this environment, we recommend combining **Q-Learning** and **Value Iteration (VI)** to create a combined RL framework that enables our algorithm agent to efficiently explore the environment and make informed decisions.\n",
       "\n",
       "Combined RL framework based on:\n",
       " - Q-learning updates the estimated action-value function according to the TD error.\n",
       "- Value Iteration improves the policy by optimizing for the value function of each state-action pair.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "start = time.time()\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "end = time.time()\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "elapsed = end - start\n",
    "print(f\"running time: {elapsed}\")\n",
    "times.append(elapsed)\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Ollama Local Model\n",
    "\n",
    "- Initialize the Ollama client pointing to the local API endpoint with the API key.  \n",
    "- Specify the model name `\"llama3.2\"`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Model Responses\n",
    "\n",
    "- Iterate over all collected answers.  \n",
    "- Concatenate them into a single string with clear section headers identifying each competitor's response.  \n",
    "- This aggregated text can be used for unified display or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation Prompt for Ranking Model Responses\n",
    "\n",
    "- Define a prompt instructing the evaluator to rank the responses from all competitors based on clarity and argument strength.  \n",
    "- Include the original task description for context.  \n",
    "- Append all competitors' responses concatenated into a single text block.  \n",
    "- Specify the desired output format: JSON listing competitors ranked from best to worst.  \n",
    "- Emphasize that the evaluator should respond **only** with JSONâ€”no extra text or formatting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = f\"\"\"You are evaluating a competition between {len(competitors)} competitors.\n",
    "Each model has been given this task:\n",
    "\n",
    "{task_description}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_messages = [{\"role\": \"user\", \"content\": evaluation}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Evaluation with the O3-Mini Model\n",
    "\n",
    "- Initialize the OpenAI client.  \n",
    "- Send the evaluation prompt (`evaluation_messages`) to the `\"o3-mini\"` model for ranking the competitors.  \n",
    "- Extract and print the JSON-formatted ranking results returned by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"2\", \"3\", \"4\", \"1\", \"5\", \"6\"]}\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=evaluation_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Displaying the Evaluation Results\n",
    "\n",
    "- Load the JSON string returned by the evaluator into a Python dictionary.  \n",
    "- Extract the ranking list from the `\"results\"` key.  \n",
    "- Iterate over the ranked competitor indices and print their rank along with the corresponding model name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.0-flash â€” Running time: 14.08 seconds\n",
      "Rank 2: deepseek-chat â€” Running time: 61.84 seconds\n",
      "Rank 3: deepseek-reasoner â€” Running time: 214.11 seconds\n",
      "Rank 4: gpt-4o-mini â€” Running time: 10.35 seconds\n",
      "Rank 5: llama-3.3-70b-versatile â€” Running time: 5.07 seconds\n",
      "Rank 6: llama3.2 â€” Running time: 43.58 seconds\n"
     ]
    }
   ],
   "source": [
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor_idx = int(result) - 1\n",
    "    competitor = competitors[competitor_idx]\n",
    "    runtime = times[competitor_idx]\n",
    "    print(f\"Rank {index+1}: {competitor} â€” Running time: {runtime:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
